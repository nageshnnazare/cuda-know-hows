/*
 * CUDA Tutorial - Part 7: Streams and Asynchronous Operations
 * 
 * This file demonstrates:
 * 1. CUDA streams for concurrent execution
 * 2. Asynchronous memory transfers
 * 3. Overlapping computation and communication
 * 4. Multi-GPU programming basics
 * 5. Performance optimization with streams
 *
 * Compile: nvcc -o streams 07_streams_async.cu
 * Run:     ./streams
 */

#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <cuda_runtime.h>

#define CUDA_CHECK(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            fprintf(stderr, "CUDA Error at %s:%d - %s\n", \
                    __FILE__, __LINE__, cudaGetErrorString(err)); \
            exit(EXIT_FAILURE); \
        } \
    } while(0)

/*
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *                        CUDA STREAMS OVERVIEW
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *
 * What is a CUDA Stream?
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * A sequence of operations that execute in order on the GPU.
 * Operations in different streams can execute concurrently.
 *
 * Default Stream (NULL stream):
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * - Synchronizes with all other streams
 * - Used when no stream is specified
 * - Can block other operations
 *
 * Non-default Streams:
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * - Can execute concurrently
 * - Enable overlap of operations
 * - Require explicit creation
 *
 * Typical Execution Timeline:
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 *
 * WITHOUT STREAMS (Sequential):
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * CPU: |H2D|Kernel|D2H|H2D|Kernel|D2H|H2D|Kernel|D2H|
 * GPU:     |Kernel|   |Kernel|   |Kernel|
 *
 * WITH STREAMS (Concurrent):
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Stream 0: |H2D|Kernel|D2H|
 * Stream 1:     |H2D|Kernel|D2H|
 * Stream 2:         |H2D|Kernel|D2H|
 * 
 * Timeline:
 * â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•
 * Copy     â”‚ S0â†’   â”‚ S1â†’   â”‚ S2â†’   â”‚       â”‚       â”‚
 * Engine   â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€
 * Compute  â”‚       â”‚  S0   â”‚  S1   â”‚  S2   â”‚       â”‚
 * Engine   â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€
 * Copy     â”‚       â”‚       â”‚  â†S0  â”‚  â†S1  â”‚  â†S2  â”‚
 * Engine   â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
 * â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•
 *
 * Benefits:
 * â”€â”€â”€â”€â”€â”€â”€â”€
 * â€¢ Hide memory transfer latency
 * â€¢ Overlap computation and communication
 * â€¢ Better GPU utilization
 * â€¢ Enable concurrent kernel execution
 */

/*
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *                    SIMPLE KERNEL FOR TESTING
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */

__global__ void vectorAdd(float *a, float *b, float *c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // Add some computation to make it more realistic
        float sum = 0.0f;
        for (int i = 0; i < 100; i++) {
            sum += sinf(a[idx]) * cosf(b[idx]);
        }
        c[idx] = a[idx] + b[idx] + sum * 0.0001f;
    }
}

__global__ void vectorScale(float *a, float scalar, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        a[idx] *= scalar;
    }
}

/*
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *                    EXAMPLE 1: BASIC STREAMS
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */

void demonstrateBasicStreams() {
    printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    printf("Example 1: Basic Stream Operations\n");
    printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n");
    
    const int N = 1 << 20;
    size_t bytes = N * sizeof(float);
    
    // Allocate pinned host memory (required for async transfers)
    float *h_a, *h_b, *h_c;
    CUDA_CHECK(cudaMallocHost(&h_a, bytes));
    CUDA_CHECK(cudaMallocHost(&h_b, bytes));
    CUDA_CHECK(cudaMallocHost(&h_c, bytes));
    
    // Initialize
    for (int i = 0; i < N; i++) {
        h_a[i] = 1.0f;
        h_b[i] = 2.0f;
    }
    
    // Allocate device memory
    float *d_a, *d_b, *d_c;
    CUDA_CHECK(cudaMalloc(&d_a, bytes));
    CUDA_CHECK(cudaMalloc(&d_b, bytes));
    CUDA_CHECK(cudaMalloc(&d_c, bytes));
    
    // Create streams
    const int nStreams = 4;
    cudaStream_t streams[nStreams];
    for (int i = 0; i < nStreams; i++) {
        CUDA_CHECK(cudaStreamCreate(&streams[i]));
    }
    
    // Events for timing
    cudaEvent_t start, stop;
    CUDA_CHECK(cudaEventCreate(&start));
    CUDA_CHECK(cudaEventCreate(&stop));
    
    int blockSize = 256;
    int gridSize = (N + blockSize - 1) / blockSize;
    
    // Test 1: Without streams (synchronous)
    printf("Test 1: Synchronous execution (no streams)\n");
    CUDA_CHECK(cudaEventRecord(start));
    
    CUDA_CHECK(cudaMemcpy(d_a, h_a, bytes, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_b, h_b, bytes, cudaMemcpyHostToDevice));
    vectorAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, N);
    CUDA_CHECK(cudaMemcpy(h_c, d_c, bytes, cudaMemcpyDeviceToHost));
    
    CUDA_CHECK(cudaEventRecord(stop));
    CUDA_CHECK(cudaEventSynchronize(stop));
    
    float syncTime = 0;
    CUDA_CHECK(cudaEventElapsedTime(&syncTime, start, stop));
    printf("  Time: %.3f ms\n\n", syncTime);
    
    // Test 2: With streams (asynchronous)
    printf("Test 2: Asynchronous execution with %d streams\n", nStreams);
    CUDA_CHECK(cudaEventRecord(start));
    
    // Launch operations in multiple streams
    for (int i = 0; i < nStreams; i++) {
        CUDA_CHECK(cudaMemcpyAsync(d_a, h_a, bytes, cudaMemcpyHostToDevice, streams[i]));
        CUDA_CHECK(cudaMemcpyAsync(d_b, h_b, bytes, cudaMemcpyHostToDevice, streams[i]));
        vectorAdd<<<gridSize, blockSize, 0, streams[i]>>>(d_a, d_b, d_c, N);
        CUDA_CHECK(cudaMemcpyAsync(h_c, d_c, bytes, cudaMemcpyDeviceToHost, streams[i]));
    }
    
    CUDA_CHECK(cudaEventRecord(stop));
    CUDA_CHECK(cudaEventSynchronize(stop));
    
    float asyncTime = 0;
    CUDA_CHECK(cudaEventElapsedTime(&asyncTime, start, stop));
    printf("  Time: %.3f ms\n", asyncTime);
    printf("  Speedup: %.2fx\n\n", syncTime / asyncTime);
    
    // Cleanup
    for (int i = 0; i < nStreams; i++) {
        CUDA_CHECK(cudaStreamDestroy(streams[i]));
    }
    CUDA_CHECK(cudaFreeHost(h_a));
    CUDA_CHECK(cudaFreeHost(h_b));
    CUDA_CHECK(cudaFreeHost(h_c));
    CUDA_CHECK(cudaFree(d_a));
    CUDA_CHECK(cudaFree(d_b));
    CUDA_CHECK(cudaFree(d_c));
    CUDA_CHECK(cudaEventDestroy(start));
    CUDA_CHECK(cudaEventDestroy(stop));
}

/*
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *              EXAMPLE 2: OVERLAPPING COMPUTE AND TRANSFER
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *
 * Strategy: Divide data into chunks and pipeline:
 * 
 * Chunk 0: |H2D|Compute|D2H|
 * Chunk 1:     |H2D|Compute|D2H|
 * Chunk 2:         |H2D|Compute|D2H|
 * Chunk 3:             |H2D|Compute|D2H|
 *
 * This hides transfer latency behind computation!
 */

void demonstratePipelining() {
    printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    printf("Example 2: Pipelining with Streams\n");
    printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n");
    
    const int N = 1 << 22;  // 4M elements
    const int nStreams = 4;
    const int chunkSize = N / nStreams;
    size_t chunkBytes = chunkSize * sizeof(float);
    
    printf("Total elements: %d\n", N);
    printf("Number of streams: %d\n", nStreams);
    printf("Elements per stream: %d\n\n", chunkSize);
    
    // Allocate pinned host memory
    float *h_a, *h_b, *h_c;
    CUDA_CHECK(cudaMallocHost(&h_a, N * sizeof(float)));
    CUDA_CHECK(cudaMallocHost(&h_b, N * sizeof(float)));
    CUDA_CHECK(cudaMallocHost(&h_c, N * sizeof(float)));
    
    // Initialize
    for (int i = 0; i < N; i++) {
        h_a[i] = sinf(i * 0.01f);
        h_b[i] = cosf(i * 0.01f);
    }
    
    // Allocate device memory
    float *d_a, *d_b, *d_c;
    CUDA_CHECK(cudaMalloc(&d_a, N * sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_b, N * sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_c, N * sizeof(float)));
    
    // Create streams
    cudaStream_t streams[nStreams];
    for (int i = 0; i < nStreams; i++) {
        CUDA_CHECK(cudaStreamCreate(&streams[i]));
    }
    
    // Events for timing
    cudaEvent_t start, stop;
    CUDA_CHECK(cudaEventCreate(&start));
    CUDA_CHECK(cudaEventCreate(&stop));
    
    int blockSize = 256;
    int gridSize = (chunkSize + blockSize - 1) / blockSize;
    
    // Pipelined execution
    printf("Executing pipelined operations...\n");
    CUDA_CHECK(cudaEventRecord(start));
    
    for (int i = 0; i < nStreams; i++) {
        int offset = i * chunkSize;
        
        // Copy chunk to device
        CUDA_CHECK(cudaMemcpyAsync(&d_a[offset], &h_a[offset], chunkBytes,
                                   cudaMemcpyHostToDevice, streams[i]));
        CUDA_CHECK(cudaMemcpyAsync(&d_b[offset], &h_b[offset], chunkBytes,
                                   cudaMemcpyHostToDevice, streams[i]));
        
        // Compute on chunk
        vectorAdd<<<gridSize, blockSize, 0, streams[i]>>>
            (&d_a[offset], &d_b[offset], &d_c[offset], chunkSize);
        
        // Copy result back
        CUDA_CHECK(cudaMemcpyAsync(&h_c[offset], &d_c[offset], chunkBytes,
                                   cudaMemcpyDeviceToHost, streams[i]));
    }
    
    CUDA_CHECK(cudaEventRecord(stop));
    CUDA_CHECK(cudaEventSynchronize(stop));
    
    float pipelineTime = 0;
    CUDA_CHECK(cudaEventElapsedTime(&pipelineTime, start, stop));
    
    printf("Pipeline execution time: %.3f ms\n", pipelineTime);
    printf("Throughput: %.2f GB/s\n\n",
           (3.0f * N * sizeof(float)) / (pipelineTime / 1000.0f) / 1e9f);
    
    // Verify results
    bool correct = true;
    for (int i = 0; i < N && i < 100; i++) {
        float expected = h_a[i] + h_b[i];
        if (fabsf(h_c[i] - expected) > 1e-3) {
            correct = false;
            break;
        }
    }
    printf("Verification: %s\n\n", correct ? "âœ“ PASSED" : "âœ— FAILED");
    
    // Cleanup
    for (int i = 0; i < nStreams; i++) {
        CUDA_CHECK(cudaStreamDestroy(streams[i]));
    }
    CUDA_CHECK(cudaFreeHost(h_a));
    CUDA_CHECK(cudaFreeHost(h_b));
    CUDA_CHECK(cudaFreeHost(h_c));
    CUDA_CHECK(cudaFree(d_a));
    CUDA_CHECK(cudaFree(d_b));
    CUDA_CHECK(cudaFree(d_c));
    CUDA_CHECK(cudaEventDestroy(start));
    CUDA_CHECK(cudaEventDestroy(stop));
}

/*
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *              EXAMPLE 3: STREAM SYNCHRONIZATION
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *
 * Different ways to synchronize streams:
 * 1. cudaStreamSynchronize(stream) - wait for specific stream
 * 2. cudaDeviceSynchronize() - wait for all streams
 * 3. cudaStreamWaitEvent() - stream waits for event
 * 4. cudaEventSynchronize() - host waits for event
 */

void demonstrateSynchronization() {
    printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    printf("Example 3: Stream Synchronization\n");
    printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n");
    
    const int N = 1 << 20;
    size_t bytes = N * sizeof(float);
    
    float *d_a, *d_b;
    CUDA_CHECK(cudaMalloc(&d_a, bytes));
    CUDA_CHECK(cudaMalloc(&d_b, bytes));
    
    cudaStream_t stream1, stream2;
    CUDA_CHECK(cudaStreamCreate(&stream1));
    CUDA_CHECK(cudaStreamCreate(&stream2));
    
    cudaEvent_t event1, event2;
    CUDA_CHECK(cudaEventCreate(&event1));
    CUDA_CHECK(cudaEventCreate(&event2));
    
    int blockSize = 256;
    int gridSize = (N + blockSize - 1) / blockSize;
    
    printf("Demonstrating stream dependencies:\n\n");
    
    // Stream 1: Scale by 2
    vectorScale<<<gridSize, blockSize, 0, stream1>>>(d_a, 2.0f, N);
    CUDA_CHECK(cudaEventRecord(event1, stream1));
    printf("  Stream 1: Scaling array by 2.0\n");
    
    // Stream 2 waits for Stream 1
    CUDA_CHECK(cudaStreamWaitEvent(stream2, event1, 0));
    vectorScale<<<gridSize, blockSize, 0, stream2>>>(d_a, 3.0f, N);
    CUDA_CHECK(cudaEventRecord(event2, stream2));
    printf("  Stream 2: Waiting for Stream 1, then scaling by 3.0\n");
    
    // Synchronize
    CUDA_CHECK(cudaEventSynchronize(event2));
    printf("  Result: Array scaled by 2.0 Ã— 3.0 = 6.0\n\n");
    
    printf("Synchronization methods:\n");
    printf("  1. cudaStreamSynchronize(stream) - Wait for one stream\n");
    printf("  2. cudaDeviceSynchronize()       - Wait for all operations\n");
    printf("  3. cudaStreamWaitEvent()         - Inter-stream dependency\n");
    printf("  4. cudaEventSynchronize()        - Wait for specific event\n\n");
    
    // Cleanup
    CUDA_CHECK(cudaStreamDestroy(stream1));
    CUDA_CHECK(cudaStreamDestroy(stream2));
    CUDA_CHECK(cudaEventDestroy(event1));
    CUDA_CHECK(cudaEventDestroy(event2));
    CUDA_CHECK(cudaFree(d_a));
    CUDA_CHECK(cudaFree(d_b));
}

/*
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *                    EXAMPLE 4: MULTI-GPU BASICS
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */

void demonstrateMultiGPU() {
    printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    printf("Example 4: Multi-GPU Programming Basics\n");
    printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n");
    
    int deviceCount;
    CUDA_CHECK(cudaGetDeviceCount(&deviceCount));
    
    printf("Number of CUDA devices: %d\n\n", deviceCount);
    
    if (deviceCount < 2) {
        printf("Multi-GPU example requires at least 2 GPUs.\n");
        printf("Showing single GPU information only.\n\n");
    }
    
    for (int i = 0; i < deviceCount && i < 4; i++) {
        cudaDeviceProp prop;
        CUDA_CHECK(cudaGetDeviceProperties(&prop, i));
        
        printf("Device %d: %s\n", i, prop.name);
        printf("  Compute Capability: %d.%d\n", prop.major, prop.minor);
        printf("  Global Memory: %.2f GB\n", 
               prop.totalGlobalMem / (1024.0f * 1024.0f * 1024.0f));
        printf("  Multiprocessors: %d\n", prop.multiProcessorCount);
        printf("  Concurrent Kernels: %s\n", 
               prop.concurrentKernels ? "Yes" : "No");
        printf("  Async Engine Count: %d\n\n", prop.asyncEngineCount);
    }
    
    if (deviceCount >= 2) {
        printf("Multi-GPU execution pattern:\n");
        printf("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");
        printf("  1. cudaSetDevice(0) - Select GPU 0\n");
        printf("  2. Allocate memory on GPU 0\n");
        printf("  3. Launch kernel on GPU 0\n");
        printf("  4. cudaSetDevice(1) - Select GPU 1\n");
        printf("  5. Allocate memory on GPU 1\n");
        printf("  6. Launch kernel on GPU 1\n");
        printf("  7. Synchronize both GPUs\n\n");
        
        printf("Advanced: Peer-to-peer transfers\n");
        printf("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");
        
        int canAccessPeer;
        CUDA_CHECK(cudaDeviceCanAccessPeer(&canAccessPeer, 0, 1));
        printf("  GPU 0 can access GPU 1: %s\n", 
               canAccessPeer ? "Yes (P2P enabled)" : "No");
        
        if (canAccessPeer) {
            printf("  Benefits: Direct GPU-to-GPU transfers\n");
            printf("  Usage: cudaMemcpyPeer()\n");
        }
    }
    printf("\n");
}

/*
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *                          MAIN PROGRAM
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */

int main(void) {
    printf("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n");
    printf("â•‘    CUDA Tutorial: Streams & Async Operations         â•‘\n");
    printf("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n");
    
    // Check device capabilities
    cudaDeviceProp prop;
    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));
    
    printf("ğŸ“Š Device: %s\n", prop.name);
    printf("   Concurrent kernels: %s\n", 
           prop.concurrentKernels ? "âœ“ Supported" : "âœ— Not supported");
    printf("   Async engine count: %d\n", prop.asyncEngineCount);
    printf("   Can map host memory: %s\n\n",
           prop.canMapHostMemory ? "âœ“ Yes" : "âœ— No");
    
    if (!prop.concurrentKernels) {
        printf("âš ï¸  Warning: This device does not support concurrent kernels.\n");
        printf("   Stream benefits will be limited.\n\n");
    }
    
    // Run examples
    demonstrateBasicStreams();
    demonstratePipelining();
    demonstrateSynchronization();
    demonstrateMultiGPU();
    
    /*
     * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     * Best Practices Summary
     * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     */
    
    printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    printf("Streams Best Practices\n");
    printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n");
    
    printf("âœ“ DO:\n");
    printf("  â€¢ Use pinned memory (cudaMallocHost) for async transfers\n");
    printf("  â€¢ Overlap data transfers with kernel execution\n");
    printf("  â€¢ Use multiple streams for independent operations\n");
    printf("  â€¢ Consider work granularity vs overhead\n");
    printf("  â€¢ Profile to verify actual concurrency\n\n");
    
    printf("âœ— DON'T:\n");
    printf("  â€¢ Use streams if operations have dependencies\n");
    printf("  â€¢ Create too many streams (overhead increases)\n");
    printf("  â€¢ Forget to destroy streams (memory leak)\n");
    printf("  â€¢ Use default stream if you need concurrency\n");
    printf("  â€¢ Assume automatic optimization (measure!)\n\n");
    
    printf("MEMORY TYPES FOR ASYNC:\n");
    printf("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");
    printf("  Type              Async Transfer    Performance\n");
    printf("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");
    printf("  Pageable (malloc) âœ— No             Standard\n");
    printf("  Pinned (cudaMallocHost) âœ“ Yes      ~2x faster\n");
    printf("  Mapped            âœ“ Yes            Special use\n");
    printf("  Managed (Unified) âœ“ Yes            Convenient\n\n");
    
    printf("STREAM PRIORITY:\n");
    printf("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");
    printf("  cudaStreamCreateWithPriority() allows setting priority\n");
    printf("  Higher priority streams get more resources\n");
    printf("  Useful for latency-critical operations\n\n");
    
    printf("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n");
    printf("â•‘                    Key Takeaways                      â•‘\n");
    printf("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n");
    printf("â•‘ 1. Streams enable concurrent GPU operations          â•‘\n");
    printf("â•‘ 2. Pinned memory is required for async transfers     â•‘\n");
    printf("â•‘ 3. Pipeline data transfers with computation          â•‘\n");
    printf("â•‘ 4. Use events for fine-grained synchronization       â•‘\n");
    printf("â•‘ 5. Profile to verify actual performance gains        â•‘\n");
    printf("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    return EXIT_SUCCESS;
}

/*
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *                         EXERCISES
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *
 * 1. Implement a producer-consumer pattern with streams
 * 2. Create a multi-GPU matrix multiplication
 * 3. Optimize image processing pipeline with streams
 * 4. Implement double buffering for continuous processing
 * 5. Measure actual overlap using NVIDIA Visual Profiler
 * 6. Compare pinned vs pageable memory performance
 *
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */

